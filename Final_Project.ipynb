{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve"
      ],
      "metadata": {
        "id": "B0eQCnrV84aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_x3d7cNUkD2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV paths\n",
        "data_path = '/content/drive/My Drive/mimic_iii_data_raw/'\n",
        "admission_path = data_path + 'ADMISSIONS.csv.gz'\n",
        "patients_path = data_path + 'PATIENTS.csv.gz'\n",
        "icu_stays_path = data_path + 'ICUSTAYS.csv.gz'\n",
        "note_events_path= data_path + 'NOTEEVENTS.csv.gz'\n",
        "chart_events_path = data_path + 'CHARTEVENTS.csv.gz'"
      ],
      "metadata": {
        "id": "hdb-57EA8VMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Patients data\n",
        "patients_df = pd.read_csv(patients_path, compression='gzip')\n",
        "patients_df['DOB'] = pd.to_datetime(patients_df['DOB'])\n",
        "patients_df = patients_df[['SUBJECT_ID', 'GENDER', 'DOB']]\n",
        "patients_df.head()"
      ],
      "metadata": {
        "id": "Mv7eNF1zCcyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Admissions data\n",
        "admissions_df = pd.read_csv(admission_path, compression='gzip')\n",
        "admissions_df['ADMITTIME'] = pd.to_datetime(admissions_df['ADMITTIME'])\n",
        "admissions_df['DISCHTIME'] = pd.to_datetime(admissions_df['DISCHTIME'])\n",
        "admissions_df['DEATHTIME'] = pd.to_datetime(admissions_df['DEATHTIME'])\n",
        "admissions_df.head()"
      ],
      "metadata": {
        "id": "Z0szbvpvGcTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge admissions and patients data\n",
        "df = pd.merge(admissions_df, patients_df, on='SUBJECT_ID', how='inner')\n",
        "\n",
        "del admissions_df\n",
        "del patients_df\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "df['AGE'] = (df['ADMITTIME'].dt.year - df['DOB'].dt.year)\n",
        "df = df[df['AGE'] >= 18]\n",
        "df = df.sort_values(by=['SUBJECT_ID', 'ADMITTIME'])\n",
        "\n",
        "df = df.drop_duplicates(subset=['SUBJECT_ID'], keep='first')\n",
        "df = df[df['HAS_CHARTEVENTS_DATA'] == 1]\n",
        "df = df.sample(n=2000, random_state=42)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "RQtajwI7HMwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns\n",
        "df['IN_HOSPITAL_MORTALITY'] = df['HOSPITAL_EXPIRE_FLAG']\n",
        "df = df[['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ADMISSION_TYPE', 'DIAGNOSIS', 'AGE', 'GENDER', 'IN_HOSPITAL_MORTALITY']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "H59BX2tKHi94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ICU data\n",
        "icustays_df = pd.read_csv(icu_stays_path, compression='gzip')\n",
        "icustays_df['INTIME'] = pd.to_datetime(icustays_df['INTIME'])\n",
        "icustays_df['OUTTIME'] = pd.to_datetime(icustays_df['OUTTIME'])\n",
        "icustays_df"
      ],
      "metadata": {
        "id": "oAdopEhUKCG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean ICU data\n",
        "cohort_icustays = pd.merge(icustays_df, df[['HADM_ID']], on=['HADM_ID'], how='inner')\n",
        "del icustays_df\n",
        "gc.collect()\n",
        "cohort_icustays = cohort_icustays.sort_values(by=['HADM_ID', 'INTIME'])\n",
        "\n",
        "first_icustays = cohort_icustays.drop_duplicates(subset=['HADM_ID'], keep='first')\n",
        "first_icustays = first_icustays[['HADM_ID', 'ICUSTAY_ID', 'INTIME', 'OUTTIME', 'LOS', 'FIRST_CAREUNIT']]\n",
        "first_icustays"
      ],
      "metadata": {
        "id": "moxOb8ZcKtSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge ICU data\n",
        "df = pd.merge(df, first_icustays, on=['HADM_ID'], how='inner')\n",
        "del first_icustays\n",
        "gc.collect()\n",
        "df"
      ],
      "metadata": {
        "id": "Rrz1kvO3LOM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process part of the chart events data\n",
        "def load_chartevents_for_cohort(chart_events_path, cohort_icustay_ids, max_chunks=1, chunk_size=5000000):\n",
        "    cohort_ids = set(cohort_icustay_ids)\n",
        "    filtered_chartevents = []\n",
        "\n",
        "    columns_to_read = ['ICUSTAY_ID', 'ITEMID', 'CHARTTIME', 'VALUE', 'VALUENUM', 'VALUEUOM']\n",
        "\n",
        "    chartevents_reader = pd.read_csv(\n",
        "        chart_events_path,\n",
        "        compression='gzip',\n",
        "        usecols=columns_to_read,\n",
        "        chunksize=chunk_size,\n",
        "        nrows=chunk_size * max_chunks\n",
        "    )\n",
        "\n",
        "    for chunk_idx, chunk in enumerate(chartevents_reader, 1):\n",
        "        start_row = (chunk_idx - 1) * chunk_size + 1\n",
        "        end_row = chunk_idx * chunk_size\n",
        "        print(f\"  Processing chartevents chunk {chunk_idx} (rows {start_row} to {end_row})...\")\n",
        "\n",
        "        relevant_records = chunk[chunk['ICUSTAY_ID'].isin(cohort_ids)]\n",
        "\n",
        "        if not relevant_records.empty:\n",
        "            filtered_chartevents.append(relevant_records)\n",
        "\n",
        "        if chunk_idx >= max_chunks:\n",
        "            print(f\"  Stopped reading CHARTEVENTS after {max_chunks} chunk(s).\")\n",
        "            break\n",
        "\n",
        "    del chartevents_reader\n",
        "    gc.collect()\n",
        "\n",
        "    return filtered_chartevents"
      ],
      "metadata": {
        "id": "iFUoURL8Odhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cohort_icustay_ids = df['ICUSTAY_ID'].unique()\n",
        "raw_chartevents_for_cohort = load_chartevents_for_cohort(chart_events_path, cohort_icustay_ids)"
      ],
      "metadata": {
        "id": "VEXiWYvoy0Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_raw_events_df = pd.concat(raw_chartevents_for_cohort, ignore_index=True)\n",
        "all_raw_events_df[\"ICUSTAY_ID\"] = all_raw_events_df[\"ICUSTAY_ID\"].astype(int)\n",
        "all_raw_events_df"
      ],
      "metadata": {
        "id": "AxmthgOUhiSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge chart events data\n",
        "df = pd.merge(df, all_raw_events_df, on='ICUSTAY_ID', how='inner')\n",
        "del all_raw_events_df\n",
        "del raw_chartevents_for_cohort\n",
        "gc.collect()\n",
        "df"
      ],
      "metadata": {
        "id": "JnPUo3dVktQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_noteevents_for_cohort(note_events_path, cohort_hadm_ids, max_chunks=1, chunk_size=500000):\n",
        "    cohort_ids = set(cohort_hadm_ids)\n",
        "    filtered_noteevents = []\n",
        "\n",
        "    columns_to_read = [\n",
        "        'ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME',\n",
        "        'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'TEXT'\n",
        "    ]\n",
        "\n",
        "    noteevents_reader = pd.read_csv(\n",
        "        note_events_path,\n",
        "        compression='gzip',\n",
        "        usecols=columns_to_read,\n",
        "        chunksize=chunk_size,\n",
        "        nrows=chunk_size * max_chunks\n",
        "    )\n",
        "\n",
        "    for chunk_idx, chunk in enumerate(noteevents_reader, 1):\n",
        "        start_row = (chunk_idx - 1) * chunk_size + 1\n",
        "        end_row = chunk_idx * chunk_size\n",
        "        print(f\"  Processing noteevents chunk {chunk_idx} (rows {start_row} to {end_row})...\")\n",
        "\n",
        "        relevant_notes = chunk[chunk['HADM_ID'].isin(cohort_ids)]\n",
        "\n",
        "        if not relevant_notes.empty:\n",
        "            filtered_noteevents.append(relevant_notes)\n",
        "\n",
        "        if chunk_idx >= max_chunks:\n",
        "            print(f\"  Stopped reading NOTEEVENTS after {max_chunks} chunk(s).\")\n",
        "            break\n",
        "\n",
        "    del noteevents_reader\n",
        "    gc.collect()\n",
        "\n",
        "    return filtered_noteevents"
      ],
      "metadata": {
        "id": "3457rcC8oVco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cohort_hadm_ids = df['HADM_ID'].unique()\n",
        "raw_noteevents_for_cohort = load_noteevents_for_cohort(note_events_path, cohort_hadm_ids)"
      ],
      "metadata": {
        "id": "yZr4J8LCzIqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_raw_notes_df = pd.concat(raw_noteevents_for_cohort, ignore_index=True)\n",
        "all_raw_notes_df[\"HADM_ID\"] = all_raw_notes_df[\"HADM_ID\"].astype(int)\n",
        "all_raw_notes_df = all_raw_notes_df[[\"HADM_ID\", \"CHARTDATE\", \"CATEGORY\", \"DESCRIPTION\", \"TEXT\"]]"
      ],
      "metadata": {
        "id": "9lqinGVavsw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge note events data\n",
        "notes_combined = all_raw_notes_df.groupby('HADM_ID').agg({\n",
        "    'TEXT': lambda x: ' '.join(str(text) for text in x if pd.notna(text))\n",
        "}).reset_index()\n",
        "df_merged = pd.merge(df, notes_combined, on='HADM_ID', how='inner')\n",
        "df_merged"
      ],
      "metadata": {
        "id": "Hm0NpGn-zQ0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create histogram of age distribution for unique admissions\n",
        "plt.figure(figsize=(10, 6))\n",
        "unique_admissions = df.drop_duplicates(subset=['HADM_ID'])\n",
        "sns.histplot(data=unique_admissions, x='AGE', kde=True, bins=20)\n",
        "plt.title('Age Distribution of Unique Admissions')\n",
        "plt.xlabel('Age (years)')\n",
        "plt.ylabel('Number of Admissions')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WqdU5kGJ84po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='GENDER', data=unique_admissions, palette=\"Set2\", hue='GENDER')\n",
        "plt.title('Gender Distribution of Unique Admissions')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Number of Admissions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nl6oQ8TsAEv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "mortality_counts = unique_admissions['IN_HOSPITAL_MORTALITY'].value_counts(normalize=True) * 100\n",
        "sns.countplot(x='IN_HOSPITAL_MORTALITY', data=unique_admissions, palette='Set2', hue='IN_HOSPITAL_MORTALITY', legend=False)\n",
        "plt.title('In-Hospital Mortality Rate (Unique Admissions)')\n",
        "plt.xlabel('In-Hospital Mortality')\n",
        "plt.ylabel('Number of Admissions')\n",
        "labels = [f'Survived ({mortality_counts.get(0,0):.1f}%)',\n",
        "          f'Expired ({mortality_counts.get(1,0):.1f}%)']\n",
        "plt.xticks([0, 1], labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sgblTWYUAMeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(y='ADMISSION_TYPE', data=unique_admissions, order=unique_admissions['ADMISSION_TYPE'].value_counts().index, palette=\"Set2\", hue=\"ADMISSION_TYPE\")\n",
        "plt.title('Admission Type Distribution (Unique Admissions)')\n",
        "plt.xlabel('Number of Admissions')\n",
        "plt.ylabel('Admission Type')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kgmQBWYrAQAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols_admissions = ['AGE', 'LOS']\n",
        "print(unique_admissions[numeric_cols_admissions].describe().T)"
      ],
      "metadata": {
        "id": "3oDtkSiYATwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.crosstab(unique_admissions['ADMISSION_TYPE'], unique_admissions['IN_HOSPITAL_MORTALITY'], margins=True, margins_name=\"Total\"))"
      ],
      "metadata": {
        "id": "qCOyDPqyAgXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del unique_admissions\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "fCxo-nF1LVVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_merged.groupby('ICUSTAY_ID').agg({\n",
        "    'VALUENUM': 'mean',\n",
        "    'TEXT': 'first',\n",
        "    'IN_HOSPITAL_MORTALITY': 'first'\n",
        "}).reset_index()\n",
        "df_final = df_final.dropna(subset=['VALUENUM', 'TEXT', 'IN_HOSPITAL_MORTALITY'])\n",
        "df_final"
      ],
      "metadata": {
        "id": "UVVlnDLaQ2i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = train_test_split(df_final, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train['VALUENUM'] = scaler.fit_transform(X_train[['VALUENUM']])\n",
        "X_test['VALUENUM'] = scaler.transform(X_test[['VALUENUM']])"
      ],
      "metadata": {
        "id": "RzxU8apjU5Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.text_char_limit = max_length * 10\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "\n",
        "        measurements = torch.tensor([row['VALUENUM']], dtype=torch.float32)\n",
        "        text = str(row['TEXT'])[:self.text_char_limit]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        label = torch.tensor(row['IN_HOSPITAL_MORTALITY'], dtype=torch.float32)\n",
        "\n",
        "        return {\n",
        "            'measurements': measurements,\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': label\n",
        "        }\n"
      ],
      "metadata": {
        "id": "6mwuFh5mVJyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MeasurementEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "foSq6kKQVMxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, hidden_dim=32, freeze_bert=True):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.projection = nn.Linear(768, hidden_dim)\n",
        "\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_token = bert_output.last_hidden_state[:, 0, :]\n",
        "        return self.projection(cls_token)"
      ],
      "metadata": {
        "id": "pRxs94YDVO_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalModel(nn.Module):\n",
        "    def __init__(self, measurement_dim=1, hidden_dim=32):\n",
        "        super(MultimodalModel, self).__init__()\n",
        "\n",
        "        self.measurement_encoder = MeasurementEncoder(measurement_dim, hidden_dim)\n",
        "        self.text_encoder = TextEncoder(hidden_dim)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, measurements, input_ids, attention_mask):\n",
        "        measurement_features = self.measurement_encoder(measurements)\n",
        "        text_features = self.text_encoder(input_ids, attention_mask)\n",
        "\n",
        "        combined_features = torch.cat([measurement_features, text_features], dim=1)\n",
        "\n",
        "        return self.classifier(combined_features)"
      ],
      "metadata": {
        "id": "oC6_KxmeVTf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_dataset = MultimodalDataset(\n",
        "    dataframe=X_train,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=64\n",
        ")\n",
        "\n",
        "test_dataset = MultimodalDataset(\n",
        "    dataframe=X_test,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=64\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "model = MultimodalModel()\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(\n",
        "    params=model.parameters(),\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-5\n",
        ")"
      ],
      "metadata": {
        "id": "rbToZYweVZn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=1):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            measurements = batch['measurements']\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['label'].unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(measurements, input_ids, attention_mask)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "897TjQPaVaWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(model, train_loader, criterion, optimizer, 10)\n",
        "torch.save(model.state_dict(), 'multimodal_model.pt')"
      ],
      "metadata": {
        "id": "HXB7W5k2rOL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_saved_model(model_path, test_dataset, batch_size=2):\n",
        "    model = MultimodalModel()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            measurements = batch['measurements']\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['label'].unsqueeze(1)\n",
        "\n",
        "            outputs = model(measurements, input_ids, attention_mask)\n",
        "\n",
        "            all_preds.extend(outputs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds).flatten()\n",
        "    all_labels = np.array(all_labels).flatten()\n",
        "\n",
        "    metrics = {\n",
        "        'auc_roc': roc_auc_score(all_labels, all_preds),\n",
        "        'auc_pr': average_precision_score(all_labels, all_preds),\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "ZMWIi4CJYnCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_shot_evaluation_saved(model_path, test_dataset, tokenizer):\n",
        "    model = MultimodalModel()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    phrases = {\n",
        "        \"positive\": \"patient deceased\",\n",
        "        \"negative\": \"discharged today\"\n",
        "    }\n",
        "\n",
        "    encodings = {}\n",
        "    for key, phrase in phrases.items():\n",
        "        encodings[key] = tokenizer(\n",
        "            phrase,\n",
        "            max_length=64,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_encoder = model.text_encoder\n",
        "        pos_emb = text_encoder(encodings[\"positive\"]['input_ids'],\n",
        "                              encodings[\"positive\"]['attention_mask'])\n",
        "        neg_emb = text_encoder(encodings[\"negative\"]['input_ids'],\n",
        "                              encodings[\"negative\"]['attention_mask'])\n",
        "\n",
        "        for batch in test_loader:\n",
        "            measurements = batch['measurements']\n",
        "            labels = batch['label']\n",
        "\n",
        "            meas_emb = model.measurement_encoder(measurements)\n",
        "\n",
        "            pos_sim = torch.nn.functional.cosine_similarity(\n",
        "                meas_emb, pos_emb.expand(meas_emb.size(0), -1), dim=1)\n",
        "            neg_sim = torch.nn.functional.cosine_similarity(\n",
        "                meas_emb, neg_emb.expand(meas_emb.size(0), -1), dim=1)\n",
        "\n",
        "            probs = torch.nn.functional.softmax(\n",
        "                torch.stack([neg_sim, pos_sim], dim=1), dim=1)\n",
        "\n",
        "            all_preds.extend(probs[:, 1].cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    metrics = {\n",
        "        'auc_roc': roc_auc_score(all_labels, all_preds),\n",
        "        'auc_pr': average_precision_score(all_labels, all_preds),\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels\n",
        "    }\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "DYoXl8CnYwba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_curves(results):\n",
        "    from sklearn.metrics import roc_curve, precision_recall_curve\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    y_pred = results['predictions']\n",
        "    y_true = results['labels']\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    axes[0].plot(fpr, tpr, label=f'AUC-ROC = {results[\"auc_roc\"]:.4f}')\n",
        "    axes[0].plot([0, 1], [0, 1], 'k--')\n",
        "    axes[0].set_xlabel('False Positive Rate')\n",
        "    axes[0].set_ylabel('True Positive Rate')\n",
        "    axes[0].set_title('ROC Curve')\n",
        "    axes[0].legend(loc='lower right')\n",
        "    axes[0].grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    axes[1].plot(recall, precision, label=f'AUC-PR = {results[\"auc_pr\"]:.4f}')\n",
        "    axes[1].set_xlabel('Recall')\n",
        "    axes[1].set_ylabel('Precision')\n",
        "    axes[1].set_title('Precision-Recall Curve')\n",
        "    axes[1].legend(loc='lower left')\n",
        "    axes[1].grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dN7UvEwsY_w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_with_paper(our_results):\n",
        "    paper_results = {\n",
        "        'supervised': {'auc_roc': 0.856, 'auc_roc_std': 0.004, 'auc_pr': 0.495, 'auc_pr_std': 0.005},\n",
        "        'zero_shot': {'auc_roc': 0.709, 'auc_pr': 0.214}\n",
        "    }\n",
        "\n",
        "    print(\"| Evaluation | AUC-ROC | Paper AUC-ROC | AUC-PR | Paper AUC-PR |\")\n",
        "    print(\"|------------|---------|---------------|--------|-------------|\")\n",
        "\n",
        "    for eval_type in ['supervised', 'zero_shot']:\n",
        "        display_name = \"Supervised\" if eval_type == 'supervised' else \"Zero-shot\"\n",
        "\n",
        "        our_roc = our_results[eval_type]['auc_roc']\n",
        "        our_pr = our_results[eval_type]['auc_pr']\n",
        "        paper_roc = paper_results[eval_type]['auc_roc']\n",
        "        paper_pr = paper_results[eval_type]['auc_pr']\n",
        "\n",
        "        print(f\"| {display_name:<11} | {our_roc:.4f} | {paper_roc:.4f} | {our_pr:.4f} | {paper_pr:.4f} |\")\n",
        "\n",
        "    sup_roc_diff = (our_results['supervised']['auc_roc'] - paper_results['supervised']['auc_roc']) / paper_results['supervised']['auc_roc'] * 100\n",
        "    sup_pr_diff = (our_results['supervised']['auc_pr'] - paper_results['supervised']['auc_pr']) / paper_results['supervised']['auc_pr'] * 100\n",
        "    zero_roc_diff = (our_results['zero_shot']['auc_roc'] - paper_results['zero_shot']['auc_roc']) / paper_results['zero_shot']['auc_roc'] * 100\n",
        "    zero_pr_diff = (our_results['zero_shot']['auc_pr'] - paper_results['zero_shot']['auc_pr']) / paper_results['zero_shot']['auc_pr'] * 100\n"
      ],
      "metadata": {
        "id": "TchGJX7BZsH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_performance():\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    test_dataset = MultimodalDataset(X_test, tokenizer, max_length=64)\n",
        "    model_path = 'multimodal_model.pt'\n",
        "\n",
        "    supervised_results = evaluate_saved_model(model_path, test_dataset)\n",
        "    plot_curves(supervised_results)\n",
        "\n",
        "    zero_shot_results = zero_shot_evaluation_saved(model_path, test_dataset, tokenizer)\n",
        "    results = {\n",
        "        'supervised': supervised_results,\n",
        "        'zero_shot': zero_shot_results\n",
        "    }\n",
        "\n",
        "    compare_with_paper(results)\n",
        "    return results"
      ],
      "metadata": {
        "id": "xN5Z0aegZwjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluate_model_performance()"
      ],
      "metadata": {
        "id": "xkN8Bt2AsgYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalModelWithDropout(nn.Module):\n",
        "    def __init__(self, measurement_dim=1, hidden_dim=32, dropout_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.measurement_encoder = MeasurementEncoder(measurement_dim, hidden_dim)\n",
        "        self.text_encoder = TextEncoder(hidden_dim)\n",
        "        self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "        self.missing_measurement_token = nn.Parameter(torch.randn(hidden_dim))\n",
        "        self.missing_text_token = nn.Parameter(torch.randn(hidden_dim))\n",
        "\n",
        "    def forward(self, measurements, input_ids, attention_mask, training=True):\n",
        "        drop_measurement = False\n",
        "        drop_text = False\n",
        "\n",
        "        if training:\n",
        "            drop_measurement = torch.rand(1).item() < self.dropout_prob\n",
        "            drop_text = torch.rand(1).item() < self.dropout_prob\n",
        "\n",
        "            if drop_measurement and drop_text:\n",
        "                drop_measurement = torch.rand(1).item() < 0.5\n",
        "                drop_text = not drop_measurement\n",
        "\n",
        "        batch_size = measurements.size(0)\n",
        "\n",
        "        measurement_emb = (\n",
        "            self.missing_measurement_token.expand(batch_size, -1)\n",
        "            if drop_measurement\n",
        "            else self.measurement_encoder(measurements)\n",
        "        )\n",
        "\n",
        "        text_emb = (\n",
        "            self.missing_text_token.expand(batch_size, -1)\n",
        "            if drop_text\n",
        "            else self.text_encoder(input_ids, attention_mask)\n",
        "        )\n",
        "\n",
        "        combined = torch.cat([measurement_emb, text_emb], dim=1)\n",
        "        return self.sigmoid(self.classifier(combined))"
      ],
      "metadata": {
        "id": "e3tT6nVbgnDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_dropout(model, train_loader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    history = {'loss': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            measurements = batch['measurements']\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['label'].unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(measurements, input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        history['loss'].append(epoch_loss)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    return model, history\n"
      ],
      "metadata": {
        "id": "cEWQzhdqgsMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_modality_settings(model, test_dataset):\n",
        "    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
        "    results = {}\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate with both modalities\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            measurements = batch['measurements']\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['label'].unsqueeze(1)\n",
        "\n",
        "            outputs = model(measurements, input_ids, attention_mask, training=False)\n",
        "\n",
        "            all_preds.extend(outputs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds).flatten()\n",
        "    all_labels = np.array(all_labels).flatten()\n",
        "\n",
        "    roc = roc_auc_score(all_labels, all_preds)\n",
        "    pr = average_precision_score(all_labels, all_preds)\n",
        "\n",
        "    results['both'] = {'auc_roc': roc, 'auc_pr': pr}\n",
        "    print(f\"Both modalities - AUC-ROC: {roc:.4f}, AUC-PR: {pr:.4f}\")\n",
        "\n",
        "    # Evaluate with only measurements\n",
        "    print(\"\\nEvaluating with only measurements...\")\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            measurements = batch['measurements']\n",
        "            input_ids = batch['input_ids']\n",
        "            labels = batch['label'].unsqueeze(1)\n",
        "\n",
        "            batch_size = input_ids.size(0)\n",
        "            text_emb = model.missing_text_token.expand(batch_size, -1)\n",
        "            measurement_emb = model.measurement_encoder(measurements)\n",
        "\n",
        "            combined = torch.cat([measurement_emb, text_emb], dim=1)\n",
        "            output = model.sigmoid(model.classifier(combined))\n",
        "\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds).flatten()\n",
        "    all_labels = np.array(all_labels).flatten()\n",
        "\n",
        "    roc = roc_auc_score(all_labels, all_preds)\n",
        "    pr = average_precision_score(all_labels, all_preds)\n",
        "\n",
        "    results['measurements_only'] = {'auc_roc': roc, 'auc_pr': pr}\n",
        "    print(f\"Measurements only - AUC-ROC: {roc:.4f}, AUC-PR: {pr:.4f}\")\n",
        "\n",
        "    # Evaluate with only text\n",
        "    print(\"\\nEvaluating with only text...\")\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            measurements = batch['measurements']\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['label'].unsqueeze(1)\n",
        "\n",
        "            batch_size = measurements.size(0)\n",
        "            measurement_emb = model.missing_measurement_token.expand(batch_size, -1)\n",
        "            text_emb = model.text_encoder(input_ids, attention_mask)\n",
        "\n",
        "            combined = torch.cat([measurement_emb, text_emb], dim=1)\n",
        "            output = model.sigmoid(model.classifier(combined))\n",
        "\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds).flatten()\n",
        "    all_labels = np.array(all_labels).flatten()\n",
        "\n",
        "    roc = roc_auc_score(all_labels, all_preds)\n",
        "    pr = average_precision_score(all_labels, all_preds)\n",
        "\n",
        "    results['text_only'] = {'auc_roc': roc, 'auc_pr': pr}\n",
        "    print(f\"Text only - AUC-ROC: {roc:.4f}, AUC-PR: {pr:.4f}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "1hIYMKSsgu_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_modality_comparison(results):\n",
        "    settings = ['both', 'measurements_only', 'text_only']\n",
        "    settings_labels = ['Both Modalities', 'Measurements Only', 'Text Only']\n",
        "\n",
        "    roc_values = [results[s]['auc_roc'] for s in settings]\n",
        "    pr_values = [results[s]['auc_pr'] for s in settings]\n",
        "\n",
        "    x = np.arange(len(settings))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    rects1 = ax.bar(x - width/2, roc_values, width, label='AUC-ROC')\n",
        "    rects2 = ax.bar(x + width/2, pr_values, width, label='AUC-PR')\n",
        "\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Performance by Modality Setting')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(settings_labels)\n",
        "    ax.legend()\n",
        "\n",
        "    for rects in (rects1, rects2):\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate(f'{height:.3f}',\n",
        "                        xy=(rect.get_x() + rect.get_width()/2, height),\n",
        "                        xytext=(0, 3),\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QFg1Ptnfg2Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_modality_dropout_extension():\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    train_dataset = MultimodalDataset(X_train, tokenizer, max_length=64)\n",
        "    test_dataset = MultimodalDataset(X_test, tokenizer, max_length=64)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    model = MultimodalModelWithDropout(dropout_prob=0.3)\n",
        "\n",
        "    original_model = MultimodalModel()\n",
        "    original_model.load_state_dict(torch.load('multimodal_model.pt'))\n",
        "\n",
        "    for target, source in zip([model.measurement_encoder, model.text_encoder],\n",
        "                             [original_model.measurement_encoder, original_model.text_encoder]):\n",
        "        target.load_state_dict(source.state_dict())\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "    model, history = train_model_with_dropout(model, train_loader, criterion,\n",
        "                                             optimizer, num_epochs=5)\n",
        "\n",
        "    torch.save(model.state_dict(), 'multimodal_model_with_dropout.pt')\n",
        "\n",
        "    modality_results = evaluate_modality_settings(model, test_dataset)\n",
        "    plot_modality_comparison(modality_results)\n",
        "\n",
        "    return modality_results\n"
      ],
      "metadata": {
        "id": "7D48nhTBg6IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modality_dropout_results = run_modality_dropout_extension()"
      ],
      "metadata": {
        "id": "VEG24lsXg9H1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}